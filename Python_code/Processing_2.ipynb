{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354b3354",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-required-modules\" data-toc-modified-id=\"Import-required-modules-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import required modules</a></span></li><li><span><a href=\"#Read-in-data\" data-toc-modified-id=\"Read-in-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Read-in data</a></span></li><li><span><a href=\"#Processing\" data-toc-modified-id=\"Processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Processing</a></span></li><li><span><a href=\"#7.1--Tokenisation\" data-toc-modified-id=\"7.1--Tokenisation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>7.1  Tokenisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interesting...\" data-toc-modified-id=\"Interesting...-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Interesting...</a></span></li></ul></li><li><span><a href=\"#Standardising\" data-toc-modified-id=\"Standardising-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Standardising</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lowercasing-our-tokens\" data-toc-modified-id=\"Lowercasing-our-tokens-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Lowercasing our tokens</a></span></li><li><span><a href=\"#Spelling-correction\" data-toc-modified-id=\"Spelling-correction-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Spelling correction</a></span></li><li><span><a href=\"#Jens-Scheuer\" data-toc-modified-id=\"Jens-Scheuer-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Jens Scheuer</a></span></li><li><span><a href=\"#Correcting-multiple-terms-in-our-'spell_checked'-column\" data-toc-modified-id=\"Correcting-multiple-terms-in-our-'spell_checked'-column-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Correcting multiple terms in our 'spell_checked' column</a></span></li><li><span><a href=\"#Other-spellcheck-issues-to-look-out-for\" data-toc-modified-id=\"Other-spellcheck-issues-to-look-out-for-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Other spellcheck issues to look out for</a></span></li></ul></li><li><span><a href=\"#Removing-Irrelevancies\" data-toc-modified-id=\"Removing-Irrelevancies-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Removing Irrelevancies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Punctuation\" data-toc-modified-id=\"Punctuation-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Punctuation</a></span></li><li><span><a href=\"#Removing-stopwords\" data-toc-modified-id=\"Removing-stopwords-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Removing stopwords</a></span></li></ul></li><li><span><a href=\"#Consolidation\" data-toc-modified-id=\"Consolidation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Consolidation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stemming-words\" data-toc-modified-id=\"Stemming-words-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Stemming words</a></span></li></ul></li><li><span><a href=\"#Basic-NLP\" data-toc-modified-id=\"Basic-NLP-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Basic NLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#PoS---Part-of-speech-tagging\" data-toc-modified-id=\"PoS---Part-of-speech-tagging-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>PoS - Part of speech tagging</a></span></li><li><span><a href=\"#Lemmatisation\" data-toc-modified-id=\"Lemmatisation-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Lemmatisation</a></span></li></ul></li><li><span><a href=\"#Extraction-prep---check-the-code\" data-toc-modified-id=\"Extraction-prep---check-the-code-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Extraction prep - check the code</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save-to-CSV\" data-toc-modified-id=\"Save-to-CSV-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Save to CSV</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970145c6",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ad381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install xlrd\n",
    "# !pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b263a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# provides functions for interacting with underlying operating system\n",
    "# e.g. change working directory, locate files\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    " # nltk stands for natural language tool kit and is useful for text-mining\n",
    "    \n",
    "import re\n",
    "# re is for regular expressions, which we use later \n",
    "\n",
    "import pandas as pd\n",
    "# includes useful functions for manipulating data \n",
    "\n",
    "import xlrd\n",
    "# we also need xlrd to read the .xls file because pandas is not old school\n",
    "\n",
    "import autocorrect\n",
    "# provides functions for spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d29e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/loucap/Documents/GitWork/Text-Mining-Health/Python_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78371120",
   "metadata": {},
   "source": [
    "## Read-in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3702d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the csv we created in the previous notebook\n",
    "# We create a variable 'df' and use pd.read_csv(filepath) to convert the csv file into a DataFrame\n",
    "df = pd.read_csv('Data/W_dataset.csv')\n",
    "df2 = pd.read_csv('Data/M_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2c4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>She_said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brighton Women name ex-Bayern coach Scheuer as...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>https://www.reuters.com/lifestyle/sports/brigh...</td>\n",
       "      <td>Dec 28 (Reuters) - Brighton &amp; Hove Albion Wome...</td>\n",
       "      <td>['I had good talks with (technical director) D...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Villa sign midfielder Nobbs from Arsenal...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>https://www.reuters.com/lifestyle/sports/aston...</td>\n",
       "      <td>Jan 5 (Reuters) - Aston Villa Women signed Jor...</td>\n",
       "      <td>[\"This is a big signing for us and Jordan is o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bethany England moves to Tottenham from Chelse...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>PA Media</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>https://www.theguardian.com/football/2023/jan/...</td>\n",
       "      <td>Bethany England has joined Tottenham from Chel...</td>\n",
       "      <td>['My next chapter. I’m so excited to join Tott...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines        Source    Author  \\\n",
       "0  Brighton Women name ex-Bayern coach Scheuer as...       Reuters       NaN   \n",
       "1  Aston Villa sign midfielder Nobbs from Arsenal...       Reuters       NaN   \n",
       "2  Bethany England moves to Tottenham from Chelse...  The Guardian  PA Media   \n",
       "\n",
       "         Date                                               Link  \\\n",
       "0  2022-12-28  https://www.reuters.com/lifestyle/sports/brigh...   \n",
       "1  2023-01-05  https://www.reuters.com/lifestyle/sports/aston...   \n",
       "2  2023-01-04  https://www.theguardian.com/football/2023/jan/...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Dec 28 (Reuters) - Brighton & Hove Albion Wome...   \n",
       "1  Jan 5 (Reuters) - Aston Villa Women signed Jor...   \n",
       "2  Bethany England has joined Tottenham from Chel...   \n",
       "\n",
       "                                              Quotes She_said  \n",
       "0  ['I had good talks with (technical director) D...      NaN  \n",
       "1  [\"This is a big signing for us and Jordan is o...      NaN  \n",
       "2  ['My next chapter. I’m so excited to join Tott...      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view the first 3 rows of the dataset\n",
    "df.head(3)\n",
    "# the default of head() is to print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e624b9",
   "metadata": {},
   "source": [
    "## Processing \n",
    "\n",
    "This includes the following steps:\n",
    "\n",
    "* Tokenisation: splitting raw data into various kinds of \"short things\" that can be statistically analysed\n",
    "* Standardising: includes converting case, correcting spelling, find-and-replace operations to remove abbreviations, RegEx etc)\n",
    "* Removing irrelevancies: includes anything from punctuation to stopwords like 'the' or 'to' that are unhelpful for many kinds of analysis\n",
    "* Consolidation: includes stemming and/or lemmatisation that strip words back to their 'root'\n",
    "* Basic NLP: includes tagging, named entity recognition, and chunking.\n",
    "\n",
    "**NOTE: In practice, most text-mining work will require that any given corpus undergo multiple steps**, but the exact steps and the exact order of steps depends on the desired analysis to be done.\n",
    "\n",
    "Also, it is good practice to create new variables whenever you manipulate an existing variable rather than write over the original. This means that you keep the original and can go back to it anytime you need to if you want to try a different manipulation or correct an error. You will see how this works as we progress through the processing steps. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d25b43",
   "metadata": {},
   "source": [
    "## 7.1  Tokenisation\n",
    "\n",
    "Our first step is to cut our 'one big thing' into tokens, or 'lots of little things'. As an example, one project I worked involved downloading a file with hundreds of recorded chess games, which I then divided into individual text files with one game each. The games had a very standard format, with every game ending with either '1-0', '0-1' or '1/2-1/2'. Thus, I was able to use regular expressions (covered in more detail later) to iterate over the file, selecting everyithing until it found an instance of '1-0', '0-1' or '1/2-1/2', at which point it would cut what it had selected, write it to a blank file, save it, and start iterating over the original file again.\n",
    "\n",
    "Other options that might make more sense with other kinds of files would be to to cut and write from the large file to new files after a specified number of lines or characters.\n",
    "\n",
    "Whether you have one big file or many smaller ones, most text-mining work will also want to divide the corpus into what are known as 'tokens'. These 'tokens' are the unit of analysis, which might be chapters, sections, paragraphs, sentences, words, or something else.\n",
    "\n",
    "Since we have our file already with one on each row, we can skip the right to tokenising that text into sentences and words. Both options are functions available through the ntlk package that we imported earlier. These are both useful tokens in their own way, so we will see how to produce both kinds.\n",
    "\n",
    "We start by dividing the text in each file into words, splitting the string into substrings whenever 'word_tokenize' detects a word.\n",
    "\n",
    "Let's try that. But this time, let's just have a look at the first 50 things it finds instead of the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543ee86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First I'll create a new column called 'tokenised_words'\n",
    "\n",
    "df['tokenised_words'] = df.apply(lambda row: nltk.word_tokenize(row['Content']), axis = 1)\n",
    "\n",
    "# apply - used to apply a function along an axis of the DataFrame: i.e, axis 1\n",
    "# lambda - anonymous function (no name) that can take any number of arguments\n",
    "# lambda ensures that the function tokenize is applied to every ROW in the text column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4192cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dec',\n",
       " '28',\n",
       " '(',\n",
       " 'Reuters',\n",
       " ')',\n",
       " '-',\n",
       " 'Brighton',\n",
       " '&',\n",
       " 'Hove',\n",
       " 'Albion',\n",
       " 'Women',\n",
       " 'have',\n",
       " 'appointed',\n",
       " 'former',\n",
       " 'Bayern',\n",
       " 'Munich',\n",
       " 'manager',\n",
       " 'Jens',\n",
       " 'Scheuer',\n",
       " 'as',\n",
       " 'their',\n",
       " 'new',\n",
       " 'head',\n",
       " 'coach',\n",
       " 'as',\n",
       " 'they',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'the',\n",
       " 'second',\n",
       " 'half',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Women',\n",
       " \"'s\",\n",
       " 'Super',\n",
       " 'League',\n",
       " '(',\n",
       " 'WSL',\n",
       " ')',\n",
       " 'season',\n",
       " ',',\n",
       " 'the',\n",
       " 'south-coast',\n",
       " 'club',\n",
       " 'said',\n",
       " 'on',\n",
       " 'Wednesday.Scheuer',\n",
       " 'will',\n",
       " 'take']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a closer look at the results, by focusing on one row of our Text column\n",
    "# Let's print the first 50 items in row 0\n",
    "\n",
    "df.tokenised_words[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5e24e",
   "metadata": {},
   "source": [
    "### Interesting...\n",
    "\n",
    "We can notice a few things.\n",
    "\n",
    "We can see that the output is a list of strings. We know it is a list because it starts and ends with square brackets and we know the things in that list are strings because they are surrounded by single quotes. \n",
    "\n",
    "We can also see that puctuation marks are counted as tokens in that list. For example, the comma within the first sentence appears as its own token because word_tokenize knows that it does not count as part of the previous word. Interestingly, 'Wednesday.Scheuer' is all one token, despite having full stops in.\n",
    "\n",
    "Nice. Now we have identified a few further things that we might want to change. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818431af",
   "metadata": {},
   "source": [
    "## Standardising \n",
    "\n",
    "This is a pretty important step if you want to look at the frequency of certain terms. In this case, we don't want a term such as 'Football' to count as a different word to 'football'. \n",
    "\n",
    "We can remove all uppercase letters with a built-in Python command, and we can use the same combination of the apply() and lambda() functions to create a new column of lowercase tokenized words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c6f54",
   "metadata": {},
   "source": [
    "### Lowercasing our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fb714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create new column with name\n",
    "df['lowercase_tokens'] = df['tokenised_words'].apply(lambda x: [t.lower() for t in x])\n",
    "\n",
    "# Then access the row we want - 'tokenised_words'\n",
    "# Use apply to apply the lambda function...\n",
    "# In this case, x is each row in our tokenised_words column\n",
    "# t is each token in the list - we perform t.lower()\n",
    "# We use for to iterate over each token in our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9457bc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan', '5', '(', 'reuters', ')', '-', 'aston', 'villa', 'women', 'signed']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it worked...\n",
    "# Looking at the first 10 elements in row 1\n",
    "df['lowercase_tokens'][1][:10]\n",
    "\n",
    "# Nice. We can see 'jan', 'aston', and 'villa' are all now lowercased!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afd579",
   "metadata": {},
   "source": [
    "### Spelling correction\n",
    "\n",
    "Another bit of standardising that we may want to do, is to ensure that we have no spelling errors. Fortunately, there are several decent spellchecking packages written for Python. \n",
    "\n",
    "They are not automatically installed and ready to import in the same way that the 'os' package was, but we just need to install the packages and import the functions we need through an installer called 'pip'. \n",
    "\n",
    "But I expect many of you are following this code through the Binder file that I put on GitHub, so you won't need to install anything using pip. If however, you want to set up your own coding environment and do some text-mining, you'll have to get familiar with pip! \n",
    "\n",
    "If you visit the top of the notebook, you can see I've put the packages you will need to pip install in a comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f17827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets access the Speller function from the package autocorrect\n",
    "# We'll assign it to a variable called 'spell'\n",
    "\n",
    "spell = autocorrect.Speller(lang = 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a0b02",
   "metadata": {},
   "source": [
    "Creating that one-word command saves us some time, which is maybe less important here but is a good skill to be aware of if you are working on text-mining every day for weeks on end. Always be on the look out for good ways to save time. \n",
    "\n",
    "Speaking of time.. to spellcheck each token contained within each row takes a few minutes, so don't worry if it takes a bit of time. Maybe a good time for a brew break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b1f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, create new column with name\n",
    "\n",
    "df['spell_checked'] = df['lowercase_tokens'].apply(lambda x: [spell(w) for w in x])\n",
    "\n",
    "# Then use combo of apply and lambda function to apply spell check to each token \n",
    "# Do this for each row in the 'lowercase_tokens' function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a429af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dec',\n",
       " '28',\n",
       " '(',\n",
       " 'reuters',\n",
       " ')',\n",
       " '-',\n",
       " 'brighton',\n",
       " '&',\n",
       " 'hove',\n",
       " 'albion',\n",
       " 'women',\n",
       " 'have',\n",
       " 'appointed',\n",
       " 'former',\n",
       " 'bayern',\n",
       " 'munich',\n",
       " 'manager',\n",
       " 'lens',\n",
       " 'scheme',\n",
       " 'as',\n",
       " 'their',\n",
       " 'new',\n",
       " 'head',\n",
       " 'coach',\n",
       " 'as',\n",
       " 'they',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'the',\n",
       " 'second',\n",
       " 'half',\n",
       " 'of',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " '(',\n",
       " 'sl',\n",
       " ')',\n",
       " 'season',\n",
       " ',',\n",
       " 'the',\n",
       " 'south-coast',\n",
       " 'club',\n",
       " 'said',\n",
       " 'on',\n",
       " 'wednesday.scheme',\n",
       " 'will',\n",
       " 'take']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['spell_checked'][0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db0d978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dec',\n",
       " '28',\n",
       " '(',\n",
       " 'reuters',\n",
       " ')',\n",
       " '-',\n",
       " 'brighton',\n",
       " '&',\n",
       " 'hove',\n",
       " 'albion',\n",
       " 'women',\n",
       " 'have',\n",
       " 'appointed',\n",
       " 'former',\n",
       " 'bayern',\n",
       " 'munich',\n",
       " 'manager',\n",
       " 'jens',\n",
       " 'scheuer',\n",
       " 'as',\n",
       " 'their',\n",
       " 'new',\n",
       " 'head',\n",
       " 'coach',\n",
       " 'as',\n",
       " 'they',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'the',\n",
       " 'second',\n",
       " 'half',\n",
       " 'of',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " '(',\n",
       " 'wsl',\n",
       " ')',\n",
       " 'season',\n",
       " ',',\n",
       " 'the',\n",
       " 'south-coast',\n",
       " 'club',\n",
       " 'said',\n",
       " 'on',\n",
       " 'wednesday.scheuer',\n",
       " 'will',\n",
       " 'take']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see if it's worked let's compare the spellchecked list above, to the original list\n",
    "\n",
    "df['lowercase_tokens'][0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8766a",
   "metadata": {},
   "source": [
    "How did it do? Well, this spell-checker replaced 'jens' 'scheuer' (the former bayern munich manager) with 'lens' 'scheme'. It can sometimes have a problem with names of organisations that are not within its dictionary. But it can also fall short with other words. This is why it's important to know your research topic well and think through if there's any other abbreviations to look out for/ensure they don't get changed by spell check. \n",
    "\n",
    "But for now, let's use RegEx to find and replace 'lens' 'scheme' with 'jens scheuer' all as one word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684ecd2",
   "metadata": {},
   "source": [
    "### Jens Scheuer\n",
    "\n",
    "Given that we know the correction of 'jens scheuer' was wrong in our row 0, let's go ahead and correct it. Of course, we might encounter more spelling errors related to these terms in other rows, but for the sake of demonstration we're only going to correct one for now. Plus, there might be times where \"scheme\" isn't referring to \"scheuer\", so we don't want to be too overhasty! \n",
    "\n",
    "**TLDR**: This is just for demonstration purposes, so you can get to grips with replacing terms using RegEx!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0c41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dictionary containing the terms we wish to correct (keys) with their replacements (values)\n",
    "replacement_words = {\"lens\": \"jens\", \"scheme\": \"scheuer\"}\n",
    "\n",
    "# specify which row we would like to correct\n",
    "row = df['spell_checked'][0]\n",
    "\n",
    "# we use a for loop to iterate over the keys and values in our dictionary\n",
    "for old, new in replacement_words.items():\n",
    "#     then we replace our old terms with new terms, whenever we encounter them in our first row\n",
    "    row = [re.sub(old, new, x) for x in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c7dfd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dec',\n",
       " '28',\n",
       " '(',\n",
       " 'reuters',\n",
       " ')',\n",
       " '-',\n",
       " 'brighton',\n",
       " '&',\n",
       " 'hove',\n",
       " 'albion',\n",
       " 'women',\n",
       " 'have',\n",
       " 'appointed',\n",
       " 'former',\n",
       " 'bayern',\n",
       " 'munich',\n",
       " 'manager',\n",
       " 'jens',\n",
       " 'scheuer',\n",
       " 'as',\n",
       " 'their',\n",
       " 'new',\n",
       " 'head',\n",
       " 'coach',\n",
       " 'as',\n",
       " 'they',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'the',\n",
       " 'second',\n",
       " 'half',\n",
       " 'of',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " '(',\n",
       " 'sl',\n",
       " ')',\n",
       " 'season',\n",
       " ',',\n",
       " 'the',\n",
       " 'south-coast',\n",
       " 'club',\n",
       " 'said',\n",
       " 'on',\n",
       " 'wednesday.scheuer',\n",
       " 'will',\n",
       " 'take',\n",
       " 'over',\n",
       " 'after',\n",
       " 'former',\n",
       " 'england',\n",
       " 'coach',\n",
       " 'hope',\n",
       " 'powell',\n",
       " 'stepped',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'october',\n",
       " 'after',\n",
       " 'five',\n",
       " 'years',\n",
       " 'in',\n",
       " 'charge.during',\n",
       " 'his',\n",
       " 'three',\n",
       " 'seasons',\n",
       " 'at',\n",
       " 'bayern',\n",
       " ',',\n",
       " 'the',\n",
       " '44-year-old',\n",
       " 'german',\n",
       " 'coach',\n",
       " 'helped',\n",
       " 'the',\n",
       " 'team',\n",
       " 'win',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'bundesliga',\n",
       " 'title',\n",
       " 'and',\n",
       " 'guided',\n",
       " 'them',\n",
       " 'to',\n",
       " 'the',\n",
       " 'semi-finals',\n",
       " 'of',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'champions',\n",
       " 'league',\n",
       " 'in',\n",
       " '2021.scheuer',\n",
       " 'will',\n",
       " 'be',\n",
       " 'tasked',\n",
       " 'with',\n",
       " 'rejuvenating',\n",
       " 'the',\n",
       " 'fortunes',\n",
       " 'of',\n",
       " 'brighton',\n",
       " ',',\n",
       " 'who',\n",
       " 'are',\n",
       " 'second-bottom',\n",
       " 'in',\n",
       " 'the',\n",
       " '12-team',\n",
       " 'sl',\n",
       " 'at',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'break',\n",
       " '.',\n",
       " '``',\n",
       " 'i',\n",
       " 'had',\n",
       " 'good',\n",
       " 'talks',\n",
       " 'with',\n",
       " '(',\n",
       " 'technical',\n",
       " 'director',\n",
       " ')',\n",
       " 'david',\n",
       " 'weir',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'we',\n",
       " 'spoke',\n",
       " 'it',\n",
       " 'was',\n",
       " 'clear',\n",
       " 'for',\n",
       " 'me',\n",
       " 'that',\n",
       " 'i',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'come',\n",
       " 'here',\n",
       " 'and',\n",
       " 'build',\n",
       " 'a',\n",
       " 'team',\n",
       " 'which',\n",
       " 'can',\n",
       " 'grow',\n",
       " 'and',\n",
       " 'move',\n",
       " 'up',\n",
       " 'the',\n",
       " 'table',\n",
       " ',',\n",
       " \"''\",\n",
       " 'said',\n",
       " 'scheuer',\n",
       " ',',\n",
       " 'who',\n",
       " 'has',\n",
       " 'agreed',\n",
       " 'a',\n",
       " 'deal',\n",
       " 'until',\n",
       " 'the',\n",
       " 'summer',\n",
       " 'of',\n",
       " '2026.our',\n",
       " 'standards',\n",
       " ':',\n",
       " 'the',\n",
       " 'thomson',\n",
       " 'reuters',\n",
       " 'trust',\n",
       " 'principles.india',\n",
       " 'captain',\n",
       " 'hit',\n",
       " 'sharma',\n",
       " 'said',\n",
       " 'he',\n",
       " 'withdrew',\n",
       " 'a',\n",
       " \"'marked\",\n",
       " \"'\",\n",
       " 'run-out',\n",
       " 'appeal',\n",
       " 'against',\n",
       " 'sun',\n",
       " 'shankar',\n",
       " 'in',\n",
       " 'tuesday',\n",
       " \"'s\",\n",
       " 'one-day',\n",
       " 'international',\n",
       " 'as',\n",
       " 'that',\n",
       " 'was',\n",
       " 'not',\n",
       " 'how',\n",
       " 'they',\n",
       " 'envisioned',\n",
       " 'dismissing',\n",
       " 'the',\n",
       " 'sri',\n",
       " 'lanka',\n",
       " 'skipper',\n",
       " ',',\n",
       " 'who',\n",
       " 'scored',\n",
       " 'a',\n",
       " 'century',\n",
       " 'in',\n",
       " 'a',\n",
       " 'game',\n",
       " 'his',\n",
       " 'team',\n",
       " 'could',\n",
       " 'not',\n",
       " 'win.reuters',\n",
       " ',',\n",
       " 'the',\n",
       " 'news',\n",
       " 'and',\n",
       " 'media',\n",
       " 'division',\n",
       " 'of',\n",
       " 'thomson',\n",
       " 'reuters',\n",
       " ',',\n",
       " 'is',\n",
       " 'the',\n",
       " 'world',\n",
       " '’',\n",
       " 's',\n",
       " 'largest',\n",
       " 'multimedia',\n",
       " 'news',\n",
       " 'provider',\n",
       " ',',\n",
       " 'reaching',\n",
       " 'billions',\n",
       " 'of',\n",
       " 'people',\n",
       " 'worldwide',\n",
       " 'every',\n",
       " 'day',\n",
       " '.',\n",
       " 'reuters',\n",
       " 'provides',\n",
       " 'business',\n",
       " ',',\n",
       " 'financial',\n",
       " ',',\n",
       " 'national',\n",
       " 'and',\n",
       " 'international',\n",
       " 'news',\n",
       " 'to',\n",
       " 'professionals',\n",
       " 'via',\n",
       " 'desktop',\n",
       " 'terminals',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " \"'s\",\n",
       " 'media',\n",
       " 'organizations',\n",
       " ',',\n",
       " 'industry',\n",
       " 'events',\n",
       " 'and',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'consumers.build',\n",
       " 'the',\n",
       " 'strongest',\n",
       " 'argument',\n",
       " 'relying',\n",
       " 'on',\n",
       " 'authoritative',\n",
       " 'content',\n",
       " ',',\n",
       " 'attorney-editor',\n",
       " 'expertise',\n",
       " ',',\n",
       " 'and',\n",
       " 'industry',\n",
       " 'defining',\n",
       " 'technology.the',\n",
       " 'most',\n",
       " 'comprehensive',\n",
       " 'solution',\n",
       " 'to',\n",
       " 'manage',\n",
       " 'all',\n",
       " 'your',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'ever-expanding',\n",
       " 'tax',\n",
       " 'and',\n",
       " 'compliance',\n",
       " 'needs.the',\n",
       " 'industry',\n",
       " 'leader',\n",
       " 'for',\n",
       " 'online',\n",
       " 'information',\n",
       " 'for',\n",
       " 'tax',\n",
       " ',',\n",
       " 'accounting',\n",
       " 'and',\n",
       " 'finance',\n",
       " 'professionals',\n",
       " '.',\n",
       " 'access',\n",
       " 'unmatched',\n",
       " 'financial',\n",
       " 'data',\n",
       " ',',\n",
       " 'news',\n",
       " 'and',\n",
       " 'content',\n",
       " 'in',\n",
       " 'a',\n",
       " 'highly-customized',\n",
       " 'workflow',\n",
       " 'experience',\n",
       " 'on',\n",
       " 'desktop',\n",
       " ',',\n",
       " 'web',\n",
       " 'and',\n",
       " 'mobile',\n",
       " '.',\n",
       " 'browse',\n",
       " 'an',\n",
       " 'unrivalled',\n",
       " 'portfolio',\n",
       " 'of',\n",
       " 'real-time',\n",
       " 'and',\n",
       " 'historical',\n",
       " 'market',\n",
       " 'data',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'worldwide',\n",
       " 'sources',\n",
       " 'and',\n",
       " 'experts.screen',\n",
       " 'for',\n",
       " 'heightened',\n",
       " 'risk',\n",
       " 'individual',\n",
       " 'and',\n",
       " 'entities',\n",
       " 'globally',\n",
       " 'to',\n",
       " 'help',\n",
       " 'uncover',\n",
       " 'hidden',\n",
       " 'risks',\n",
       " 'in',\n",
       " 'business',\n",
       " 'relationships',\n",
       " 'and',\n",
       " 'human',\n",
       " 'networks.all',\n",
       " 'quotes',\n",
       " 'delayed',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'of',\n",
       " '15',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'see',\n",
       " 'here',\n",
       " 'for',\n",
       " 'a',\n",
       " 'complete',\n",
       " 'list',\n",
       " 'of',\n",
       " 'exchanges',\n",
       " 'and',\n",
       " 'delays.©',\n",
       " '2022',\n",
       " 'reuters',\n",
       " '.',\n",
       " 'all',\n",
       " 'rights',\n",
       " 'reserved']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it worked...\n",
    "\n",
    "row\n",
    "\n",
    "# Nice! Onto the next one..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ab4c8",
   "metadata": {},
   "source": [
    "### Correcting multiple terms in our 'spell_checked' column\n",
    "\n",
    "Okay, let's now say that we did want to correct more than one term, but for all of the rows in the spell_checked column. To do so, we could create a function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c08e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we could create a function and then apply it to the 'spell_checked' column\n",
    "\n",
    "# First we create a function, using the 'def' keyword\n",
    "# Then we create a parameter called 'x' - this is what we supply an argument to, when we call the function\n",
    "def replacement_mapping(x):\n",
    "#     if statement - means if string item is equal to 'der' do this...\n",
    "        if x == \"lens\":\n",
    "#         substitute the \"der\" string for \"defra\"\n",
    "            return re.sub(\"lens\",\"jens\",x)\n",
    "        elif x == \"scheme\":\n",
    "            return re.sub(\"scheme\",\"scheuer\",x)\n",
    "        else:\n",
    "#         Otherwise return x, i.e., don't change the string\n",
    "            return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24bf65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and apply this function to the 'spell_checked' column\n",
    "\n",
    "df[\"spell_checked\"] = df[\"spell_checked\"].apply(lambda x:[replacement_mapping(w) for w in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c954d92",
   "metadata": {},
   "source": [
    "### Other spellcheck issues to look out for\n",
    "\n",
    "When applying the Speller function to your text, you might want to look our for import abbreviations. For instance, in this women's dataset, we might expect after we have lowercased our text and done some spell checking, that \"WSL\" (which would now be \"wsl\") becomes a different term. This is something you'll have to keep in mind when performing text-mining. It might be handy to explore a few of the articles first, to identify if there will be any important accronymns that you want to preserve. Once you have your list, you can use the same function above to correct any terms that will be contextually useful and crucial to your analysis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18231b3d",
   "metadata": {},
   "source": [
    "## Removing Irrelevancies \n",
    "\n",
    "Depending on your analysis, you may or may not want to remove some things that are present in text but irrelevant to your purposes. Let's start with punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518bfb2e",
   "metadata": {},
   "source": [
    "### Punctuation \n",
    "\n",
    "Punctuation is not always very useful for understanding text, especially if you look at words as tokens because lots of the punctuation ends up being tokenised on its own. \n",
    "\n",
    "We could use RegEx to replace all punctuation with nothing, and that is a valid approach. But, just for variety sake, I will demonstrate another way here. We can define a string that includes all the standard English language punctuation, and then use that to iterate over the relevant DataFrame column, removing anything that matches.\n",
    "\n",
    "But wait... Do we really want to remove the:\n",
    "\n",
    "- hyphens in things like 'ninety-six' or words like 'lactose-free'? \n",
    "- full stops in things like 'u.k.'? \n",
    "- the apostrophe in contractions or possessives?\n",
    "\n",
    "There are no right or wrong answers here. Every project will have to decide, based on the research questions, what is the right choice for the specific context. In this case, we want to remove the full stops, even from 'u.k.' so that it becomes identical to 'uk'. \n",
    "\n",
    "But, at the same time, we don't necessarily want to remove apostrophes. That is a punctuation mark that occurs in the middle of words and do add meaning to the word, so I want to keep them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07dfc572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”-\n"
     ]
    }
   ],
   "source": [
    "# First, let's define a variable with all the punctuation to remove.\n",
    "English_punctuation = \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”-\" \n",
    " # Print that defined variable, just to check it is correct.\n",
    "print(English_punctuation)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35c94f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function with a parameter 'from_text'\n",
    "\n",
    "def remove_punctuation(from_text):\n",
    "    # The function 'maketrans' creates a table that maps the punctuation marks to 'None'\n",
    "    table = str.maketrans('', '', English_punctuation) \n",
    "    stripped = [w.translate(table) for w in from_text]\n",
    "    return stripped\n",
    "\n",
    "# We use the above function to iterate over the strings in a row\n",
    "df['no_punct'] = [remove_punctuation(i) for i in df['lowercase_tokens']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5bfc4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan',\n",
       " '5',\n",
       " '',\n",
       " 'reuters',\n",
       " '',\n",
       " '',\n",
       " 'aston',\n",
       " 'villa',\n",
       " 'women',\n",
       " 'signed',\n",
       " 'jordan',\n",
       " 'nobbs',\n",
       " 'from',\n",
       " 'arsenal',\n",
       " 'on',\n",
       " 'thursday',\n",
       " '',\n",
       " 'with',\n",
       " 'the',\n",
       " 'england',\n",
       " 'midfielder',\n",
       " 'joining',\n",
       " 'on',\n",
       " 'an',\n",
       " '18month',\n",
       " 'contract',\n",
       " 'with',\n",
       " 'an',\n",
       " 'option',\n",
       " 'to',\n",
       " 'extendnobbs',\n",
       " 'joined',\n",
       " 'arsenal',\n",
       " 'in',\n",
       " '2010',\n",
       " 'and',\n",
       " 'won',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " '',\n",
       " 'wsl',\n",
       " '',\n",
       " 'on',\n",
       " 'three',\n",
       " 'occasions',\n",
       " 'during',\n",
       " 'her']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it actually removed the punctuation...\n",
    "\n",
    "df['no_punct'][1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2dd31fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan',\n",
       " '5',\n",
       " '(',\n",
       " 'reuters',\n",
       " ')',\n",
       " '-',\n",
       " 'aston',\n",
       " 'villa',\n",
       " 'women',\n",
       " 'signed',\n",
       " 'jordan',\n",
       " 'nobbs',\n",
       " 'from',\n",
       " 'arsenal',\n",
       " 'on',\n",
       " 'thursday',\n",
       " ',',\n",
       " 'with',\n",
       " 'the',\n",
       " 'england',\n",
       " 'midfielder',\n",
       " 'joining',\n",
       " 'on',\n",
       " 'an',\n",
       " '18-month',\n",
       " 'contract',\n",
       " 'with',\n",
       " 'an',\n",
       " 'option',\n",
       " 'to',\n",
       " 'extend.nobbs',\n",
       " 'joined',\n",
       " 'arsenal',\n",
       " 'in',\n",
       " '2010',\n",
       " 'and',\n",
       " 'won',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " '(',\n",
       " 'wsl',\n",
       " ')',\n",
       " 'on',\n",
       " 'three',\n",
       " 'occasions',\n",
       " 'during',\n",
       " 'her']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compare it to row 1 of the column without punctuation removed...\n",
    "df['lowercase_tokens'][1][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d05d4",
   "metadata": {},
   "source": [
    "#### But... what about the empty strings?\n",
    "\n",
    "Did you notice that removing the punctuation has left list items that are empty strings. Between 'thursday' and 'with', for example, is an item shown as ''. This is an empty string item that was a full stop before we removed the punctuation. \n",
    "\n",
    "Since those empty strings are python-recognised instances of 'None',  python can find and filter them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5883ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new column with a new name 'no_punct_no_space'\n",
    "# We use list comprehension to filter each instance of 'None' and remove it from each row\n",
    "\n",
    "df['no_punct_no_space'] = [list(filter(None, sublist)) for sublist in df['no_punct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38f369f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan',\n",
       " '5',\n",
       " 'reuters',\n",
       " 'aston',\n",
       " 'villa',\n",
       " 'women',\n",
       " 'signed',\n",
       " 'jordan',\n",
       " 'nobbs',\n",
       " 'from',\n",
       " 'arsenal',\n",
       " 'on',\n",
       " 'thursday',\n",
       " 'with',\n",
       " 'the',\n",
       " 'england',\n",
       " 'midfielder',\n",
       " 'joining',\n",
       " 'on',\n",
       " 'an',\n",
       " '18month',\n",
       " 'contract',\n",
       " 'with',\n",
       " 'an',\n",
       " 'option',\n",
       " 'to',\n",
       " 'extendnobbs',\n",
       " 'joined',\n",
       " 'arsenal',\n",
       " 'in',\n",
       " '2010',\n",
       " 'and',\n",
       " 'won',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " 'wsl',\n",
       " 'on',\n",
       " 'three',\n",
       " 'occasions',\n",
       " 'during',\n",
       " 'her',\n",
       " '12year',\n",
       " 'stay',\n",
       " 'at',\n",
       " 'the',\n",
       " 'london',\n",
       " 'club']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it's worked..\n",
    "\n",
    "df['no_punct_no_space'][1][:50]\n",
    "# Nice. We have no empty string items!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbec70f",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "Stopwords are typically conjunctions ('and', 'or'), prepositions ('to', 'around'), determiners ('the', 'an'), possessives ('s) and the like. The are **REALLY** common in all languages, and tend to occur at about the same ratio in all kinds of writing, regardless of who did the writing or what it is about. These words are definitely important for structure as they make all the difference between \"Freeze *or* I'll shoot!\" and \"Freeze *and* I'll shoot!\". \n",
    "\n",
    "Buuuut... for many text-mining analyses, these words don't have a whole lot of meaning in and of themselves. Thus, we want to remove them. \n",
    "\n",
    "Let's start by downloading the basic stopwords function built into nltk and storing the English language ones in a list called, appropriately enough, 'stop_words'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "912ad3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/loucap/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fafc235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# create a stop_words variable that contains all of the english stopwords in a list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# the sorted() function makes sure the list is arranged in alphabetical order\n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf9bac",
   "metadata": {},
   "source": [
    "Ah! It contains quite a few words that will change the meaning of a sentence. So I will make sure to customise the stopwords list so they DON'T include these important words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05e76769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new variable to contain our new_stopwords\n",
    "\n",
    "# We preserve the original stop_words UNLESS they match the strings in the brackets\n",
    "new_stopwords =[e for e in stop_words if e not in \n",
    "                (\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\n",
    "                 \"isn't\",\"mightn't\",\"mustn't\",\"needn't\",'no','not',\"only\",\"shouldn't\",\"wasn't\",\n",
    "                 \"weren't\",\"won't\",\"wouldn't\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bcb03",
   "metadata": {},
   "source": [
    "Great. Now let's remove those new_stopwords by creating another column called `no_stop_words`. Then, we iterate over the `no_punct_no_space` column, looking at them one by one and appending them to `no_stop_words` if and only if they do not match any of the items in the stop_words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ff15548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column called 'no_stop_words'\n",
    "df['no_stop_words'] = df['no_punct_no_space'].apply(lambda x: [item for item in x if item not in new_stopwords])\n",
    "\n",
    "# Use apply and lambda combo\n",
    "# Basically, keep item in x as item it is, if it's not a stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f9bff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan',\n",
       " '5',\n",
       " 'reuters',\n",
       " 'aston',\n",
       " 'villa',\n",
       " 'women',\n",
       " 'signed',\n",
       " 'jordan',\n",
       " 'nobbs',\n",
       " 'arsenal',\n",
       " 'thursday',\n",
       " 'england',\n",
       " 'midfielder',\n",
       " 'joining',\n",
       " '18month',\n",
       " 'contract',\n",
       " 'option',\n",
       " 'extendnobbs',\n",
       " 'joined',\n",
       " 'arsenal',\n",
       " '2010',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " 'wsl',\n",
       " 'three',\n",
       " 'occasions',\n",
       " '12year',\n",
       " 'stay',\n",
       " 'london',\n",
       " 'club',\n",
       " 'also',\n",
       " 'four',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'fa',\n",
       " 'cups',\n",
       " 'five',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'league',\n",
       " 'cupsthe',\n",
       " '30yearold',\n",
       " '69',\n",
       " 'caps',\n",
       " 'england',\n",
       " 'left',\n",
       " 'sarina',\n",
       " 'wiegman']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You know the drill, let's see if it worked...\n",
    "\n",
    "df['no_stop_words'][1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "701f568c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan',\n",
       " '5',\n",
       " 'reuters',\n",
       " 'aston',\n",
       " 'villa',\n",
       " 'women',\n",
       " 'signed',\n",
       " 'jordan',\n",
       " 'nobbs',\n",
       " 'from',\n",
       " 'arsenal',\n",
       " 'on',\n",
       " 'thursday',\n",
       " 'with',\n",
       " 'the',\n",
       " 'england',\n",
       " 'midfielder',\n",
       " 'joining',\n",
       " 'on',\n",
       " 'an',\n",
       " '18month',\n",
       " 'contract',\n",
       " 'with',\n",
       " 'an',\n",
       " 'option',\n",
       " 'to',\n",
       " 'extendnobbs',\n",
       " 'joined',\n",
       " 'arsenal',\n",
       " 'in',\n",
       " '2010',\n",
       " 'and',\n",
       " 'won',\n",
       " 'the',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'league',\n",
       " 'wsl',\n",
       " 'on',\n",
       " 'three',\n",
       " 'occasions',\n",
       " 'during',\n",
       " 'her',\n",
       " '12year',\n",
       " 'stay',\n",
       " 'at',\n",
       " 'the',\n",
       " 'london',\n",
       " 'club']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['no_punct_no_space'][1][:50]\n",
    "\n",
    "# If we compare this to the above code we can see that we've got rid of 'and' \n",
    "# and 'the', along with other stop words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a01447",
   "metadata": {},
   "source": [
    "**REMINDER:** Make sure to always convert to lowercase before trying this!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e8eedd0",
   "metadata": {},
   "source": [
    "## Consolidation\n",
    "\n",
    "This includes includes stemming and/or lemmatisation that strip words back to their 'root'.\n",
    "\n",
    "### Stemming words\n",
    "\n",
    "You can probably imagine what comes next by now. We import a specific tool from nltk (it is not called the natural language tool kit for nuthin') and apply it to a pre-existing column! \n",
    "\n",
    "Stemming aggressively strips back word markers, like verb endings and plurals in sort of very basic way using rules like “remove ed from words that end in ed or remove s from words that end in s.” Putting our text \n",
    "through a stemmer returns a new list with plurals and verb endings removed. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c37820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2dd60431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have our Stemming function\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# First we create our new column\n",
    "df['stemmed'] = df['no_stop_words'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "\n",
    "# Then we apply the stemming function to each word 'y' in our row list, 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21c205f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jan',\n",
       " '5',\n",
       " 'reuter',\n",
       " 'aston',\n",
       " 'villa',\n",
       " 'women',\n",
       " 'sign',\n",
       " 'jordan',\n",
       " 'nobb',\n",
       " 'arsen',\n",
       " 'thursday',\n",
       " 'england',\n",
       " 'midfield',\n",
       " 'join',\n",
       " '18month',\n",
       " 'contract',\n",
       " 'option',\n",
       " 'extendnobb',\n",
       " 'join',\n",
       " 'arsen',\n",
       " '2010',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'super',\n",
       " 'leagu',\n",
       " 'wsl',\n",
       " 'three',\n",
       " 'occas',\n",
       " '12year',\n",
       " 'stay',\n",
       " 'london',\n",
       " 'club',\n",
       " 'also',\n",
       " 'four',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'fa',\n",
       " 'cup',\n",
       " 'five',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'leagu',\n",
       " 'cupsth',\n",
       " '30yearold',\n",
       " '69',\n",
       " 'cap',\n",
       " 'england',\n",
       " 'left',\n",
       " 'sarina',\n",
       " 'wiegman']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if it's worked...\n",
    "\n",
    "df['stemmed'][1][:50]\n",
    "\n",
    "# Yep! Some examples...\n",
    "# signed --> sign\n",
    "# joined --> join\n",
    "# midfielder --> midfield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8240d",
   "metadata": {},
   "source": [
    "#### Let's move onto lemmatisation...right?\n",
    "\n",
    "Not so fast! \n",
    "\n",
    "The main idea behind lemmatisation is to group different inflected forms of a word into one. For example, go, going, gone and went will become just one - go. But to derive this, lemmatisation would have to know the context of a word - whether the word is a noun or verb etc. But without part of speech tagging, everything by default will be treated like a noun. So in order for lemmatisation to work effectively, we must first know the context of a word!\n",
    "\n",
    "I.e., let's move onto **BASIC NLP** before we lemmatise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff141ab2",
   "metadata": {},
   "source": [
    "## Basic NLP\n",
    "\n",
    "This includes: \n",
    "\n",
    "* Tagging - part of speech (PoS) tagging is mentioned above, and it is the process of converting a list of words into a list of tuples (where each tuple has the form (word, tag). The tag signifies whether the word is a noun, adjective, verb, and so on. \n",
    "* Named Entity Recognition -  seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "* Chunking - Chunking is a process of extracting phrases from unstructured text, which means analyzing a sentence to identify the constituents(Noun Groups, Verbs, verb groups, etc.) However, it does not specify their internal structure, nor their role in the main sentence. It works on top of POS tagging to extract phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3cd3be",
   "metadata": {},
   "source": [
    "### PoS - Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0974381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jan', 'NN'),\n",
       " ('5', 'CD'),\n",
       " ('reuters', 'NNS'),\n",
       " ('aston', 'VBP'),\n",
       " ('villa', 'JJ'),\n",
       " ('women', 'NNS'),\n",
       " ('signed', 'VBD'),\n",
       " ('jordan', 'NN'),\n",
       " ('nobbs', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('arsenal', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('thursday', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('england', 'NN'),\n",
       " ('midfielder', 'NN'),\n",
       " ('joining', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('18month', 'CD'),\n",
       " ('contract', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('option', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('extendnobbs', 'VB'),\n",
       " ('joined', 'JJ'),\n",
       " ('arsenal', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('2010', 'CD'),\n",
       " ('and', 'CC'),\n",
       " ('won', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('women', 'NNS'),\n",
       " (\"'s\", 'POS'),\n",
       " ('super', 'JJ'),\n",
       " ('league', 'NN'),\n",
       " ('wsl', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('three', 'CD'),\n",
       " ('occasions', 'NNS'),\n",
       " ('during', 'IN'),\n",
       " ('her', 'PRP$'),\n",
       " ('12year', 'CD'),\n",
       " ('stay', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('london', 'NN'),\n",
       " ('club', 'NN')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column called 'pos_tag'\n",
    "df['pos_tag'] = df['no_punct_no_space'].apply(lambda x: nltk.pos_tag(x))\n",
    "# Apply nltk.pos_tag() function to each row\n",
    "# nltk.pos_tag() works on a list of strings\n",
    "\n",
    "# Let's see if it worked\n",
    "df['pos_tag'][1][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a7f3c",
   "metadata": {},
   "source": [
    "Excellent. That has successfully added POS tags to all off the words in our dataframe. Now, let's try lemmatising again with the POS tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070654cb",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "Lemmatisation is similar to stemming, in that it aims to turn various forms of the same word into a single form. However, lemmatisation is a bit more sophisticated because: \n",
    "\n",
    "- It recognises irregular plurals and returns the correct singular form. Example = 'rocks' --> 'rock' but 'corpora' --> 'corpus' \n",
    "- If part of speech tags are supplied, it treats verbs, adjectives and nouns differenly, even if they have the same surface form. Example - 'caring' would not be changed if used as an adjective (as in 'his caring manner') but would go to 'care' if it was a verb (as in 'he is caring for baby squirrels'. In contrast, stemming would remove the 'ing' and turn 'caring' into 'car'. \n",
    "- If no part of speech tags are supplied, lemmatisation tools tend to assume words as nouns, so the process becomes a sophisticated de-pluraliser. \n",
    "\n",
    "This is better for this research because since we will be looking into the meaning of the data, it will need to put into the most accurate base form as possible, if I were to stem this, a lot of words would lose meaning!\n",
    "\n",
    "This way we get the extra efficiency of base-form transformation while keeping the mmeaning intact!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "653b1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer          \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "from nltk.corpus import wordnet    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97999c0c",
   "metadata": {},
   "source": [
    "Now despite what seems obvious, the nltk POS tagger does not use the same POS tags that the nltk lemmatize function needs. Why? I have no idea. \n",
    "\n",
    "But to move forward, I need a to define a quick little function called get_wordnet_pos to convert the tag format to the right one. I tell a lie. I did not write this function but copied it off of Stack Overflow. This is not cheating so much as being economical. A HUGE number of the things you want to do or the problems you want to solve will be discussed on Stack Overflow. Just use a popular search engine to find them, read through all the answers, try them out. \n",
    "\n",
    "Having defined the get_wordnet_pos function, the code below then creates a new, blank column called `lemmatised`. \n",
    "After that, the code iterates over the `pos_tagged` column, looking at each word and POS-tag pair in each row, using the get_wordnet_pos function to convert the POS-tag to the right format, and using that to lemmatize correctly. \n",
    "\n",
    "At the end, the lemmatised word is appended to the new column we created. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c7e7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function with 'word' parameter\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     We're converting all our PoS tags into wordnet tags\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "df['lemmatised'] = df['pos_tag'].apply(lambda x: [lemmatizer.lemmatize(y[0], get_wordnet_pos(y[0])) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3086042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dec', '28', 'reuters', 'brighton', 'hove', 'albion', 'woman', 'have', 'appoint', 'former', 'bayern', 'munich', 'manager', 'jens', 'scheuer', 'a', 'their', 'new', 'head', 'coach', 'a', 'they', 'prepare', 'for', 'the', 'second', 'half', 'of', 'the', 'woman', \"'s\", 'super', 'league', 'wsl', 'season', 'the', 'southcoast', 'club', 'say', 'on', 'wednesdayscheuer', 'will', 'take', 'over', 'after', 'former', 'england', 'coach', 'hope', 'powell']\n"
     ]
    }
   ],
   "source": [
    "# Has lemmatisation worked?\n",
    "\n",
    "print(df['lemmatised'][0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2db228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dec', 'NN'), ('28', 'CD'), ('reuters', 'NNS'), ('brighton', 'VBP'), ('hove', 'VB'), ('albion', 'NN'), ('women', 'NNS'), ('have', 'VBP'), ('appointed', 'VBN'), ('former', 'JJ'), ('bayern', 'NN'), ('munich', 'NN'), ('manager', 'NN'), ('jens', 'VBZ'), ('scheuer', 'NN'), ('as', 'IN'), ('their', 'PRP$'), ('new', 'JJ'), ('head', 'NN'), ('coach', 'NN'), ('as', 'IN'), ('they', 'PRP'), ('prepare', 'VBP'), ('for', 'IN'), ('the', 'DT'), ('second', 'JJ'), ('half', 'NN'), ('of', 'IN'), ('the', 'DT'), ('women', 'NNS'), (\"'s\", 'POS'), ('super', 'JJ'), ('league', 'NN'), ('wsl', 'NN'), ('season', 'NN'), ('the', 'DT'), ('southcoast', 'NN'), ('club', 'NN'), ('said', 'VBD'), ('on', 'IN'), ('wednesdayscheuer', 'NN'), ('will', 'MD'), ('take', 'VB'), ('over', 'RP'), ('after', 'IN'), ('former', 'JJ'), ('england', 'NN'), ('coach', 'NN'), ('hope', 'VBP'), ('powell', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(df['pos_tag'][0][:50])\n",
    "\n",
    "# Yes! We can see that 'women' has been changed to 'woman',\n",
    "# and 'appointed' has been changed to 'appoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4c546",
   "metadata": {},
   "source": [
    "## Extraction prep - check the code\n",
    "\n",
    "We're essentially done with the processing stage now. But we have a number of columns, and it would be a bit messy to look at if we were to use all of them. So let's just take the 'no_stop_words' column for now.\n",
    "\n",
    "NOTE: When conducting your own text-mining project, there's no need (ofc) to create every single column as we have done. This has mainly been done for demonstration purposes. It's going to be up to you and your research aims to decide which pre-processing + processing steps you need to apply to build your perfect DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4326e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>She_said</th>\n",
       "      <th>tokenised_words</th>\n",
       "      <th>lowercase_tokens</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_punct_no_space</th>\n",
       "      <th>no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brighton Women name ex-Bayern coach Scheuer as...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>https://www.reuters.com/lifestyle/sports/brigh...</td>\n",
       "      <td>Dec 28 (Reuters) - Brighton &amp; Hove Albion Wome...</td>\n",
       "      <td>['I had good talks with (technical director) D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Dec, 28, (, Reuters, ), -, Brighton, &amp;, Hove,...</td>\n",
       "      <td>[dec, 28, (, reuters, ), -, brighton, &amp;, hove,...</td>\n",
       "      <td>[dec, 28, (, reuters, ), -, brighton, &amp;, hove,...</td>\n",
       "      <td>[dec, 28, , reuters, , , brighton, , hove, alb...</td>\n",
       "      <td>[dec, 28, reuters, brighton, hove, albion, wom...</td>\n",
       "      <td>[dec, 28, reuters, brighton, hove, albion, wom...</td>\n",
       "      <td>[dec, 28, reuter, brighton, hove, albion, wome...</td>\n",
       "      <td>[(dec, NN), (28, CD), (reuters, NNS), (brighto...</td>\n",
       "      <td>[dec, 28, reuters, brighton, hove, albion, wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Villa sign midfielder Nobbs from Arsenal...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>https://www.reuters.com/lifestyle/sports/aston...</td>\n",
       "      <td>Jan 5 (Reuters) - Aston Villa Women signed Jor...</td>\n",
       "      <td>[\"This is a big signing for us and Jordan is o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Jan, 5, (, Reuters, ), -, Aston, Villa, Women...</td>\n",
       "      <td>[jan, 5, (, reuters, ), -, aston, villa, women...</td>\n",
       "      <td>[jan, 5, (, reuters, ), -, aston, villa, women...</td>\n",
       "      <td>[jan, 5, , reuters, , , aston, villa, women, s...</td>\n",
       "      <td>[jan, 5, reuters, aston, villa, women, signed,...</td>\n",
       "      <td>[jan, 5, reuters, aston, villa, women, signed,...</td>\n",
       "      <td>[jan, 5, reuter, aston, villa, women, sign, jo...</td>\n",
       "      <td>[(jan, NN), (5, CD), (reuters, NNS), (aston, V...</td>\n",
       "      <td>[jan, 5, reuters, aston, villa, woman, sign, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bethany England moves to Tottenham from Chelse...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>PA Media</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>https://www.theguardian.com/football/2023/jan/...</td>\n",
       "      <td>Bethany England has joined Tottenham from Chel...</td>\n",
       "      <td>['My next chapter. I’m so excited to join Tott...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Bethany, England, has, joined, Tottenham, fro...</td>\n",
       "      <td>[bethany, england, has, joined, tottenham, fro...</td>\n",
       "      <td>[botany, england, has, joined, tottenham, from...</td>\n",
       "      <td>[bethany, england, has, joined, tottenham, fro...</td>\n",
       "      <td>[bethany, england, has, joined, tottenham, fro...</td>\n",
       "      <td>[bethany, england, joined, tottenham, chelsea,...</td>\n",
       "      <td>[bethani, england, join, tottenham, chelsea, r...</td>\n",
       "      <td>[(bethany, NN), (england, NN), (has, VBZ), (jo...</td>\n",
       "      <td>[bethany, england, have, join, tottenham, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemma Bonner rejoins Liverpool – decades after...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Guardian sport</td>\n",
       "      <td>2022-12-24</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>Liverpool Women have re-signed the defender Ge...</td>\n",
       "      <td>['a computer and a football Liverpool kit and ...</td>\n",
       "      <td>On Saturday she said: “Growing up as a Liverpo...</td>\n",
       "      <td>[Liverpool, Women, have, re-signed, the, defen...</td>\n",
       "      <td>[liverpool, women, have, re-signed, the, defen...</td>\n",
       "      <td>[liverpool, women, have, re-signed, the, defen...</td>\n",
       "      <td>[liverpool, women, have, resigned, the, defend...</td>\n",
       "      <td>[liverpool, women, have, resigned, the, defend...</td>\n",
       "      <td>[liverpool, women, resigned, defender, gemma, ...</td>\n",
       "      <td>[liverpool, women, resign, defend, gemma, bonn...</td>\n",
       "      <td>[(liverpool, JJ), (women, NNS), (have, VBP), (...</td>\n",
       "      <td>[liverpool, woman, have, resign, the, defender...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top women footballers are being pushed past br...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Suzanne Wrack</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>Beth Mead is among a host of players sidelined...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Beth, Mead, is, among, a, host, of, players, ...</td>\n",
       "      <td>[beth, mead, is, among, a, host, of, players, ...</td>\n",
       "      <td>[beth, mead, is, among, a, host, of, players, ...</td>\n",
       "      <td>[beth, mead, is, among, a, host, of, players, ...</td>\n",
       "      <td>[beth, mead, is, among, a, host, of, players, ...</td>\n",
       "      <td>[beth, mead, among, host, players, sidelined, ...</td>\n",
       "      <td>[beth, mead, among, host, player, sidelin, acl...</td>\n",
       "      <td>[(beth, NNS), (mead, NN), (is, VBZ), (among, I...</td>\n",
       "      <td>[beth, mead, be, among, a, host, of, player, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines        Source  \\\n",
       "0  Brighton Women name ex-Bayern coach Scheuer as...       Reuters   \n",
       "1  Aston Villa sign midfielder Nobbs from Arsenal...       Reuters   \n",
       "2  Bethany England moves to Tottenham from Chelse...  The Guardian   \n",
       "3  Gemma Bonner rejoins Liverpool – decades after...  The Guardian   \n",
       "4  Top women footballers are being pushed past br...  The Guardian   \n",
       "\n",
       "           Author        Date  \\\n",
       "0             NaN  2022-12-28   \n",
       "1             NaN  2023-01-05   \n",
       "2        PA Media  2023-01-04   \n",
       "3  Guardian sport  2022-12-24   \n",
       "4   Suzanne Wrack  2022-12-22   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.reuters.com/lifestyle/sports/brigh...   \n",
       "1  https://www.reuters.com/lifestyle/sports/aston...   \n",
       "2  https://www.theguardian.com/football/2023/jan/...   \n",
       "3  https://www.theguardian.com/football/2022/dec/...   \n",
       "4  https://www.theguardian.com/football/2022/dec/...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Dec 28 (Reuters) - Brighton & Hove Albion Wome...   \n",
       "1  Jan 5 (Reuters) - Aston Villa Women signed Jor...   \n",
       "2  Bethany England has joined Tottenham from Chel...   \n",
       "3  Liverpool Women have re-signed the defender Ge...   \n",
       "4  Beth Mead is among a host of players sidelined...   \n",
       "\n",
       "                                              Quotes  \\\n",
       "0  ['I had good talks with (technical director) D...   \n",
       "1  [\"This is a big signing for us and Jordan is o...   \n",
       "2  ['My next chapter. I’m so excited to join Tott...   \n",
       "3  ['a computer and a football Liverpool kit and ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                            She_said  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  On Saturday she said: “Growing up as a Liverpo...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     tokenised_words  \\\n",
       "0  [Dec, 28, (, Reuters, ), -, Brighton, &, Hove,...   \n",
       "1  [Jan, 5, (, Reuters, ), -, Aston, Villa, Women...   \n",
       "2  [Bethany, England, has, joined, Tottenham, fro...   \n",
       "3  [Liverpool, Women, have, re-signed, the, defen...   \n",
       "4  [Beth, Mead, is, among, a, host, of, players, ...   \n",
       "\n",
       "                                    lowercase_tokens  \\\n",
       "0  [dec, 28, (, reuters, ), -, brighton, &, hove,...   \n",
       "1  [jan, 5, (, reuters, ), -, aston, villa, women...   \n",
       "2  [bethany, england, has, joined, tottenham, fro...   \n",
       "3  [liverpool, women, have, re-signed, the, defen...   \n",
       "4  [beth, mead, is, among, a, host, of, players, ...   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  [dec, 28, (, reuters, ), -, brighton, &, hove,...   \n",
       "1  [jan, 5, (, reuters, ), -, aston, villa, women...   \n",
       "2  [botany, england, has, joined, tottenham, from...   \n",
       "3  [liverpool, women, have, re-signed, the, defen...   \n",
       "4  [beth, mead, is, among, a, host, of, players, ...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  [dec, 28, , reuters, , , brighton, , hove, alb...   \n",
       "1  [jan, 5, , reuters, , , aston, villa, women, s...   \n",
       "2  [bethany, england, has, joined, tottenham, fro...   \n",
       "3  [liverpool, women, have, resigned, the, defend...   \n",
       "4  [beth, mead, is, among, a, host, of, players, ...   \n",
       "\n",
       "                                   no_punct_no_space  \\\n",
       "0  [dec, 28, reuters, brighton, hove, albion, wom...   \n",
       "1  [jan, 5, reuters, aston, villa, women, signed,...   \n",
       "2  [bethany, england, has, joined, tottenham, fro...   \n",
       "3  [liverpool, women, have, resigned, the, defend...   \n",
       "4  [beth, mead, is, among, a, host, of, players, ...   \n",
       "\n",
       "                                       no_stop_words  \\\n",
       "0  [dec, 28, reuters, brighton, hove, albion, wom...   \n",
       "1  [jan, 5, reuters, aston, villa, women, signed,...   \n",
       "2  [bethany, england, joined, tottenham, chelsea,...   \n",
       "3  [liverpool, women, resigned, defender, gemma, ...   \n",
       "4  [beth, mead, among, host, players, sidelined, ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [dec, 28, reuter, brighton, hove, albion, wome...   \n",
       "1  [jan, 5, reuter, aston, villa, women, sign, jo...   \n",
       "2  [bethani, england, join, tottenham, chelsea, r...   \n",
       "3  [liverpool, women, resign, defend, gemma, bonn...   \n",
       "4  [beth, mead, among, host, player, sidelin, acl...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(dec, NN), (28, CD), (reuters, NNS), (brighto...   \n",
       "1  [(jan, NN), (5, CD), (reuters, NNS), (aston, V...   \n",
       "2  [(bethany, NN), (england, NN), (has, VBZ), (jo...   \n",
       "3  [(liverpool, JJ), (women, NNS), (have, VBP), (...   \n",
       "4  [(beth, NNS), (mead, NN), (is, VBZ), (among, I...   \n",
       "\n",
       "                                          lemmatised  \n",
       "0  [dec, 28, reuters, brighton, hove, albion, wom...  \n",
       "1  [jan, 5, reuters, aston, villa, woman, sign, j...  \n",
       "2  [bethany, england, have, join, tottenham, from...  \n",
       "3  [liverpool, woman, have, resign, the, defender...  \n",
       "4  [beth, mead, be, among, a, host, of, player, s...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at our dataframe which contains each of our processed columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e68cd8",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We have achieved a lot! Now you can move onto the extraction stage! \n",
    "\n",
    "**NOTE:** We can follow the same steps for the Men's Premier League dataset too. For the sake of demonstration purposes I have only processed the Women's Super League dataset, but I will include the code below for the men's dataset so that you can execute it in your own time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75508cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>He_said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brentford v Liverpool: Premier League – live</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Barry Glendenning</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>https://www.theguardian.com/football/live/2023...</td>\n",
       "      <td>Brentford ran out more than worthy winners aga...</td>\n",
       "      <td>['It was a wild game and a game Brentford won ...</td>\n",
       "      <td>”Was he concerned about how his players would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Premier League wants to introduce temporary co...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Paul MacInnes</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>The Premier League wants to adopt the use of t...</td>\n",
       "      <td>['progressive development', 'Headway has repea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American businessman Foley completes takeover ...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>https://www.reuters.com/lifestyle/sports/ameri...</td>\n",
       "      <td>Dec 13 (Reuters) - Bournemouth have been taken...</td>\n",
       "      <td>[\"Bill is committed to increased investment in...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines        Source  \\\n",
       "0       Brentford v Liverpool: Premier League – live  The Guardian   \n",
       "1  Premier League wants to introduce temporary co...  The Guardian   \n",
       "2  American businessman Foley completes takeover ...       Reuters   \n",
       "\n",
       "              Author        Date  \\\n",
       "0  Barry Glendenning  2023-01-02   \n",
       "1      Paul MacInnes  2022-12-21   \n",
       "2                NaN  2022-12-13   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.theguardian.com/football/live/2023...   \n",
       "1  https://www.theguardian.com/football/2022/dec/...   \n",
       "2  https://www.reuters.com/lifestyle/sports/ameri...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Brentford ran out more than worthy winners aga...   \n",
       "1  The Premier League wants to adopt the use of t...   \n",
       "2  Dec 13 (Reuters) - Bournemouth have been taken...   \n",
       "\n",
       "                                              Quotes  \\\n",
       "0  ['It was a wild game and a game Brentford won ...   \n",
       "1  ['progressive development', 'Headway has repea...   \n",
       "2  [\"Bill is committed to increased investment in...   \n",
       "\n",
       "                                             He_said  \n",
       "0  ”Was he concerned about how his players would ...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2a9df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tokenised_words'] = df2.apply(lambda row: nltk.word_tokenize(row['Content']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "284219f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['lowercase_tokens'] = df2['tokenised_words'].apply(lambda x: [t.lower() for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f368e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['spell_checked'] = df2['lowercase_tokens'].apply(lambda x: [spell(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf5daa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['no_punct'] = [remove_punctuation(i) for i in df2['lowercase_tokens']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a4b75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['no_punct_no_space'] = [list(filter(None, sublist)) for sublist in df2['no_punct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df92c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['no_stop_words'] = df2['no_punct_no_space'].apply(lambda x: [item for item in x if item not in new_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f87f56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['stemmed'] = df2['no_stop_words'].apply(lambda x: [porter.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e58b313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['pos_tag'] = df2['no_punct_no_space'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7cfe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['lemmatised'] = df2['pos_tag'].apply(lambda x: [lemmatizer.lemmatize(y[0], get_wordnet_pos(y[0])) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbdae5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>He_said</th>\n",
       "      <th>tokenised_words</th>\n",
       "      <th>lowercase_tokens</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_punct_no_space</th>\n",
       "      <th>no_stop_words</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brentford v Liverpool: Premier League – live</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Barry Glendenning</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>https://www.theguardian.com/football/live/2023...</td>\n",
       "      <td>Brentford ran out more than worthy winners aga...</td>\n",
       "      <td>['It was a wild game and a game Brentford won ...</td>\n",
       "      <td>”Was he concerned about how his players would ...</td>\n",
       "      <td>[Brentford, ran, out, more, than, worthy, winn...</td>\n",
       "      <td>[brentford, ran, out, more, than, worthy, winn...</td>\n",
       "      <td>[brentford, ran, out, more, than, worthy, winn...</td>\n",
       "      <td>[brentford, ran, out, more, than, worthy, winn...</td>\n",
       "      <td>[brentford, ran, out, more, than, worthy, winn...</td>\n",
       "      <td>[brentford, ran, worthy, winners, exalted, oft...</td>\n",
       "      <td>[brentford, ran, worthi, winner, exalt, often,...</td>\n",
       "      <td>[(brentford, NN), (ran, VBD), (out, RP), (more...</td>\n",
       "      <td>[brentford, ran, out, more, than, worthy, winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Premier League wants to introduce temporary co...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Paul MacInnes</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>The Premier League wants to adopt the use of t...</td>\n",
       "      <td>['progressive development', 'Headway has repea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[The, Premier, League, wants, to, adopt, the, ...</td>\n",
       "      <td>[the, premier, league, wants, to, adopt, the, ...</td>\n",
       "      <td>[the, premier, league, wants, to, adopt, the, ...</td>\n",
       "      <td>[the, premier, league, wants, to, adopt, the, ...</td>\n",
       "      <td>[the, premier, league, wants, to, adopt, the, ...</td>\n",
       "      <td>[premier, league, wants, adopt, use, temporary...</td>\n",
       "      <td>[premier, leagu, want, adopt, use, temporari, ...</td>\n",
       "      <td>[(the, DT), (premier, JJR), (league, NN), (wan...</td>\n",
       "      <td>[the, premier, league, want, to, adopt, the, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American businessman Foley completes takeover ...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>https://www.reuters.com/lifestyle/sports/ameri...</td>\n",
       "      <td>Dec 13 (Reuters) - Bournemouth have been taken...</td>\n",
       "      <td>[\"Bill is committed to increased investment in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Dec, 13, (, Reuters, ), -, Bournemouth, have,...</td>\n",
       "      <td>[dec, 13, (, reuters, ), -, bournemouth, have,...</td>\n",
       "      <td>[dec, 13, (, reuters, ), -, bournemouth, have,...</td>\n",
       "      <td>[dec, 13, , reuters, , , bournemouth, have, be...</td>\n",
       "      <td>[dec, 13, reuters, bournemouth, have, been, ta...</td>\n",
       "      <td>[dec, 13, reuters, bournemouth, taken, black, ...</td>\n",
       "      <td>[dec, 13, reuter, bournemouth, taken, black, k...</td>\n",
       "      <td>[(dec, NN), (13, CD), (reuters, NNS), (bournem...</td>\n",
       "      <td>[dec, 13, reuters, bournemouth, have, be, take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crystal Palace v Tottenham: Premier League – live</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Will Unwin</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>https://www.theguardian.com/football/live/2023...</td>\n",
       "      <td>A Harry Kane double helps leads Spurs to a dom...</td>\n",
       "      <td>['It was a good reaction. It is not easy to wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A, Harry, Kane, double, helps, leads, Spurs, ...</td>\n",
       "      <td>[a, harry, kane, double, helps, leads, spurs, ...</td>\n",
       "      <td>[a, harry, kane, double, helps, leads, spurs, ...</td>\n",
       "      <td>[a, harry, kane, double, helps, leads, spurs, ...</td>\n",
       "      <td>[a, harry, kane, double, helps, leads, spurs, ...</td>\n",
       "      <td>[harry, kane, double, helps, leads, spurs, dom...</td>\n",
       "      <td>[harri, kane, doubl, help, lead, spur, domin, ...</td>\n",
       "      <td>[(a, DT), (harry, NN), (kane, NN), (double, JJ...</td>\n",
       "      <td>[a, harry, kane, double, help, lead, spur, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assistant referee Bhupinder Singh Gill to make...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Paul MacInnes</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>https://www.theguardian.com/football/2023/jan/...</td>\n",
       "      <td>Bhupinder Singh Gill says he hopes to “inspire...</td>\n",
       "      <td>['inspire the next generation', 'This has to b...</td>\n",
       "      <td>“I go back with their dad for quite some time...</td>\n",
       "      <td>[Bhupinder, Singh, Gill, says, he, hopes, to, ...</td>\n",
       "      <td>[bhupinder, singh, gill, says, he, hopes, to, ...</td>\n",
       "      <td>[bhupinder, singh, gill, says, he, hopes, to, ...</td>\n",
       "      <td>[bhupinder, singh, gill, says, he, hopes, to, ...</td>\n",
       "      <td>[bhupinder, singh, gill, says, he, hopes, to, ...</td>\n",
       "      <td>[bhupinder, singh, gill, says, hopes, inspire,...</td>\n",
       "      <td>[bhupind, singh, gill, say, hope, inspir, next...</td>\n",
       "      <td>[(bhupinder, NN), (singh, NN), (gill, NN), (sa...</td>\n",
       "      <td>[bhupinder, singh, gill, say, he, hope, to, in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines        Source  \\\n",
       "0       Brentford v Liverpool: Premier League – live  The Guardian   \n",
       "1  Premier League wants to introduce temporary co...  The Guardian   \n",
       "2  American businessman Foley completes takeover ...       Reuters   \n",
       "3  Crystal Palace v Tottenham: Premier League – live  The Guardian   \n",
       "4  Assistant referee Bhupinder Singh Gill to make...  The Guardian   \n",
       "\n",
       "              Author        Date  \\\n",
       "0  Barry Glendenning  2023-01-02   \n",
       "1      Paul MacInnes  2022-12-21   \n",
       "2                NaN  2022-12-13   \n",
       "3         Will Unwin  2023-01-04   \n",
       "4      Paul MacInnes  2023-01-04   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.theguardian.com/football/live/2023...   \n",
       "1  https://www.theguardian.com/football/2022/dec/...   \n",
       "2  https://www.reuters.com/lifestyle/sports/ameri...   \n",
       "3  https://www.theguardian.com/football/live/2023...   \n",
       "4  https://www.theguardian.com/football/2023/jan/...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Brentford ran out more than worthy winners aga...   \n",
       "1  The Premier League wants to adopt the use of t...   \n",
       "2  Dec 13 (Reuters) - Bournemouth have been taken...   \n",
       "3  A Harry Kane double helps leads Spurs to a dom...   \n",
       "4  Bhupinder Singh Gill says he hopes to “inspire...   \n",
       "\n",
       "                                              Quotes  \\\n",
       "0  ['It was a wild game and a game Brentford won ...   \n",
       "1  ['progressive development', 'Headway has repea...   \n",
       "2  [\"Bill is committed to increased investment in...   \n",
       "3  ['It was a good reaction. It is not easy to wi...   \n",
       "4  ['inspire the next generation', 'This has to b...   \n",
       "\n",
       "                                             He_said  \\\n",
       "0  ”Was he concerned about how his players would ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4   “I go back with their dad for quite some time...   \n",
       "\n",
       "                                     tokenised_words  \\\n",
       "0  [Brentford, ran, out, more, than, worthy, winn...   \n",
       "1  [The, Premier, League, wants, to, adopt, the, ...   \n",
       "2  [Dec, 13, (, Reuters, ), -, Bournemouth, have,...   \n",
       "3  [A, Harry, Kane, double, helps, leads, Spurs, ...   \n",
       "4  [Bhupinder, Singh, Gill, says, he, hopes, to, ...   \n",
       "\n",
       "                                    lowercase_tokens  \\\n",
       "0  [brentford, ran, out, more, than, worthy, winn...   \n",
       "1  [the, premier, league, wants, to, adopt, the, ...   \n",
       "2  [dec, 13, (, reuters, ), -, bournemouth, have,...   \n",
       "3  [a, harry, kane, double, helps, leads, spurs, ...   \n",
       "4  [bhupinder, singh, gill, says, he, hopes, to, ...   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  [brentford, ran, out, more, than, worthy, winn...   \n",
       "1  [the, premier, league, wants, to, adopt, the, ...   \n",
       "2  [dec, 13, (, reuters, ), -, bournemouth, have,...   \n",
       "3  [a, harry, kane, double, helps, leads, spurs, ...   \n",
       "4  [bhupinder, singh, gill, says, he, hopes, to, ...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  [brentford, ran, out, more, than, worthy, winn...   \n",
       "1  [the, premier, league, wants, to, adopt, the, ...   \n",
       "2  [dec, 13, , reuters, , , bournemouth, have, be...   \n",
       "3  [a, harry, kane, double, helps, leads, spurs, ...   \n",
       "4  [bhupinder, singh, gill, says, he, hopes, to, ...   \n",
       "\n",
       "                                   no_punct_no_space  \\\n",
       "0  [brentford, ran, out, more, than, worthy, winn...   \n",
       "1  [the, premier, league, wants, to, adopt, the, ...   \n",
       "2  [dec, 13, reuters, bournemouth, have, been, ta...   \n",
       "3  [a, harry, kane, double, helps, leads, spurs, ...   \n",
       "4  [bhupinder, singh, gill, says, he, hopes, to, ...   \n",
       "\n",
       "                                       no_stop_words  \\\n",
       "0  [brentford, ran, worthy, winners, exalted, oft...   \n",
       "1  [premier, league, wants, adopt, use, temporary...   \n",
       "2  [dec, 13, reuters, bournemouth, taken, black, ...   \n",
       "3  [harry, kane, double, helps, leads, spurs, dom...   \n",
       "4  [bhupinder, singh, gill, says, hopes, inspire,...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [brentford, ran, worthi, winner, exalt, often,...   \n",
       "1  [premier, leagu, want, adopt, use, temporari, ...   \n",
       "2  [dec, 13, reuter, bournemouth, taken, black, k...   \n",
       "3  [harri, kane, doubl, help, lead, spur, domin, ...   \n",
       "4  [bhupind, singh, gill, say, hope, inspir, next...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(brentford, NN), (ran, VBD), (out, RP), (more...   \n",
       "1  [(the, DT), (premier, JJR), (league, NN), (wan...   \n",
       "2  [(dec, NN), (13, CD), (reuters, NNS), (bournem...   \n",
       "3  [(a, DT), (harry, NN), (kane, NN), (double, JJ...   \n",
       "4  [(bhupinder, NN), (singh, NN), (gill, NN), (sa...   \n",
       "\n",
       "                                          lemmatised  \n",
       "0  [brentford, ran, out, more, than, worthy, winn...  \n",
       "1  [the, premier, league, want, to, adopt, the, u...  \n",
       "2  [dec, 13, reuters, bournemouth, have, be, take...  \n",
       "3  [a, harry, kane, double, help, lead, spur, to,...  \n",
       "4  [bhupinder, singh, gill, say, he, hope, to, in...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb76bf8",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09f1b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/W_processed.csv', index = False)\n",
    "df2.to_csv('Data/M_processed.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
