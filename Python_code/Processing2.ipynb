{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354b3354",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-required-modules\" data-toc-modified-id=\"Import-required-modules-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import required modules</a></span></li><li><span><a href=\"#Read-in-data\" data-toc-modified-id=\"Read-in-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Read-in data</a></span></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-processing</a></span></li><li><span><a href=\"#Removing-+-renaming-some-columns\" data-toc-modified-id=\"Removing-+-renaming-some-columns-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Removing + renaming some columns</a></span></li><li><span><a href=\"#Processing\" data-toc-modified-id=\"Processing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenisation\" data-toc-modified-id=\"Tokenisation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Tokenisation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970145c6",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ad381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install xlrd\n",
    "# !pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b263a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# provides functions for interacting with underlying operating system\n",
    "# e.g. change working directory, locate files\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    " # nltk stands for natural language tool kit and is useful for text-mining\n",
    "    \n",
    "import re\n",
    "# re is for regular expressions, which we use later \n",
    "\n",
    "import pandas as pd\n",
    "# includes useful functions for manipulating data \n",
    "\n",
    "import xlrd\n",
    "# we also need xlrd to read the .xls file because pandas is not old school\n",
    "\n",
    "import autocorrect\n",
    "# provides functions for spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d29e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/loucap/Documents/GitWork/Text-Mining-Health/Python_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78371120",
   "metadata": {},
   "source": [
    "## Read-in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3702d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the csv we created in the previous notebook\n",
    "# We create a variable 'df' and use pd.read_csv(filepath) to convert the csv file into a DataFrame\n",
    "df = pd.read_csv('Data/Womens_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2c4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Women’s Super League: talking points from the ...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Suzanne Wrack, Sophie Downey and Sarah Rendell</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>https://www.theguardian.com/football/2022/nov/...</td>\n",
       "      <td>Arsenal’s winning run was ended in dramatic st...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Moving the Goalposts | ‘We want to keep dreami...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Sophie Downey</td>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>Club has soared since acquiring the Serie A li...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Leicester’s Ashleigh Plumptre: ‘I love everyth...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Ella Braidwood</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>https://www.theguardian.com/football/2022/nov/...</td>\n",
       "      <td>The defender on playing for the club in her he...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Man City win keeps pressure on WSL top three</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04</td>\n",
       "      <td>https://www.bbc.co.uk/sport/football/63771047</td>\n",
       "      <td>Last updated on 4 December 20224 December 2022...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Shaw stars for Man City in WSL win at Everton</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>https://www.bbc.co.uk/sport/football/63606404</td>\n",
       "      <td>Last updated on 19 November 202219 November 20...</td>\n",
       "      <td>['focal point', 'She does so much more than sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          Headlines  \\\n",
       "0           0  Women’s Super League: talking points from the ...   \n",
       "1           1  Moving the Goalposts | ‘We want to keep dreami...   \n",
       "2           2  Leicester’s Ashleigh Plumptre: ‘I love everyth...   \n",
       "3           3       Man City win keeps pressure on WSL top three   \n",
       "4           4      Shaw stars for Man City in WSL win at Everton   \n",
       "\n",
       "         Source                                          Author        Date  \\\n",
       "0  The Guardian  Suzanne Wrack, Sophie Downey and Sarah Rendell  2022-11-21   \n",
       "1  The Guardian                                   Sophie Downey  2022-12-07   \n",
       "2  The Guardian                                  Ella Braidwood  2022-11-17   \n",
       "3      BBC News                                             NaN  2022-12-04   \n",
       "4      BBC News                                             NaN  2022-11-19   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.theguardian.com/football/2022/nov/...   \n",
       "1  https://www.theguardian.com/football/2022/dec/...   \n",
       "2  https://www.theguardian.com/football/2022/nov/...   \n",
       "3      https://www.bbc.co.uk/sport/football/63771047   \n",
       "4      https://www.bbc.co.uk/sport/football/63606404   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Arsenal’s winning run was ended in dramatic st...   \n",
       "1  Club has soared since acquiring the Serie A li...   \n",
       "2  The defender on playing for the club in her he...   \n",
       "3  Last updated on 4 December 20224 December 2022...   \n",
       "4  Last updated on 19 November 202219 November 20...   \n",
       "\n",
       "                                              Quotes  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4  ['focal point', 'She does so much more than sc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view the first 5 rows of the dataset\n",
    "df.head(5)\n",
    "# the default of head() is to print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f3c83",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Data Preprocessing is a technique which is used to convert the raw data set into a clean data set. In other words, whenever the data is collected from different sources it is collected in raw format which is not feasible for the analysis.\n",
    "\n",
    "Hence, certain steps are followed and executed in order to convert the data into a small and clean data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910eb77d",
   "metadata": {},
   "source": [
    "## Removing + renaming some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31cb869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women’s Super League: talking points from the ...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Suzanne Wrack, Sophie Downey and Sarah Rendell</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>https://www.theguardian.com/football/2022/nov/...</td>\n",
       "      <td>Arsenal’s winning run was ended in dramatic st...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Moving the Goalposts | ‘We want to keep dreami...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Sophie Downey</td>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>Club has soared since acquiring the Serie A li...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leicester’s Ashleigh Plumptre: ‘I love everyth...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Ella Braidwood</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>https://www.theguardian.com/football/2022/nov/...</td>\n",
       "      <td>The defender on playing for the club in her he...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man City win keeps pressure on WSL top three</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04</td>\n",
       "      <td>https://www.bbc.co.uk/sport/football/63771047</td>\n",
       "      <td>Last updated on 4 December 20224 December 2022...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaw stars for Man City in WSL win at Everton</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>https://www.bbc.co.uk/sport/football/63606404</td>\n",
       "      <td>Last updated on 19 November 202219 November 20...</td>\n",
       "      <td>['focal point', 'She does so much more than sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines        Source  \\\n",
       "0  Women’s Super League: talking points from the ...  The Guardian   \n",
       "1  Moving the Goalposts | ‘We want to keep dreami...  The Guardian   \n",
       "2  Leicester’s Ashleigh Plumptre: ‘I love everyth...  The Guardian   \n",
       "3       Man City win keeps pressure on WSL top three      BBC News   \n",
       "4      Shaw stars for Man City in WSL win at Everton      BBC News   \n",
       "\n",
       "                                           Author        Date  \\\n",
       "0  Suzanne Wrack, Sophie Downey and Sarah Rendell  2022-11-21   \n",
       "1                                   Sophie Downey  2022-12-07   \n",
       "2                                  Ella Braidwood  2022-11-17   \n",
       "3                                             NaN  2022-12-04   \n",
       "4                                             NaN  2022-11-19   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.theguardian.com/football/2022/nov/...   \n",
       "1  https://www.theguardian.com/football/2022/dec/...   \n",
       "2  https://www.theguardian.com/football/2022/nov/...   \n",
       "3      https://www.bbc.co.uk/sport/football/63771047   \n",
       "4      https://www.bbc.co.uk/sport/football/63606404   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Arsenal’s winning run was ended in dramatic st...   \n",
       "1  Club has soared since acquiring the Serie A li...   \n",
       "2  The defender on playing for the club in her he...   \n",
       "3  Last updated on 4 December 20224 December 2022...   \n",
       "4  Last updated on 19 November 202219 November 20...   \n",
       "\n",
       "                                              Quotes  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4  ['focal point', 'She does so much more than sc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't need the first column 'Unnamed: 0', as our rows already have a numbered index\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e624b9",
   "metadata": {},
   "source": [
    "## Processing \n",
    "\n",
    "This includes the following steps:\n",
    "\n",
    "* Tokenisation: splitting raw data into various kinds of \"short things\" that can be statistically analysed\n",
    "* Standardising: includes converting case, correcting spelling, find-and-replace operations to remove abbreviations, RegEx etc)\n",
    "* Removing irrelevancies: includes anything from punctuation to stopwords like 'the' or 'to' that are unhelpful for many kinds of analysis\n",
    "* Consolidation: includes stemming and/or lemmatisation that strip words back to their 'root'\n",
    "* Basic NLP: includes tagging, named entity recognition, and chunking.\n",
    "\n",
    "NOTE: In practice, most text-mining work will require that any given corpus undergo multiple steps, but the exact steps and the exact order of steps depends on the desired analysis to be done.\n",
    "\n",
    "Also, it is good practice to create new variables whenever you manipulate an existing variable rather than write over the original. This means that you keep the original and can go back to it anytime you need to if you want to try a different manipulation or correct an error. You will see how this works as we progress through the processing steps. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538512c",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "Our first step is to cut our 'one big thing' into tokens, or 'lots of little things'. As an example, one project I worked involved downloading a file with hundreds of recorded chess games, which I then divided into individual text files with one game each. The games had a very standard format, with every game ending with either '1-0', '0-1' or '1/2-1/2'. Thus, I was able to use regular expressions (covered in more detail later) to iterate over the file, selecting everyithing until it found an instance of '1-0', '0-1' or '1/2-1/2', at which point it would cut what it had selected, write it to a blank file, save it, and start iterating over the original file again. \n",
    "\n",
    "Other options that might make more sense with other kinds of files would be to to cut and write from the large file to new files after a specified number of lines or characters. \n",
    "\n",
    "Whether you have one big file or many smaller ones, most text-mining work will also want to divide the corpus into what are known as 'tokens'. These 'tokens' are the unit of analysis, which might be chapters, sections, paragraphs, sentences, words, or something else. \n",
    "\n",
    "Since we have our file already with one on each row, we can skip the right to tokenising that text into sentences and words. Both options are functions available through the ntlk package that we imported earlier. These are both useful tokens in their own way, so we will see how to produce both kinds. \n",
    " \n",
    "We start by dividing the text in each file into words, splitting the string into substrings whenever 'word_tokenize' detects a word. \n",
    "\n",
    "Let's try that. But this time, let's just have a look at the first 100 things it finds instead of the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778a173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Source</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women’s Super League: talking points from the ...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Suzanne Wrack, Sophie Downey and Sarah Rendell</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>https://www.theguardian.com/football/2022/nov/...</td>\n",
       "      <td>Arsenal’s winning run was ended in dramatic st...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Moving the Goalposts | ‘We want to keep dreami...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Sophie Downey</td>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>https://www.theguardian.com/football/2022/dec/...</td>\n",
       "      <td>Club has soared since acquiring the Serie A li...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leicester’s Ashleigh Plumptre: ‘I love everyth...</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Ella Braidwood</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>https://www.theguardian.com/football/2022/nov/...</td>\n",
       "      <td>The defender on playing for the club in her he...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man City win keeps pressure on WSL top three</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04</td>\n",
       "      <td>https://www.bbc.co.uk/sport/football/63771047</td>\n",
       "      <td>Last updated on 4 December 20224 December 2022...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaw stars for Man City in WSL win at Everton</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>https://www.bbc.co.uk/sport/football/63606404</td>\n",
       "      <td>Last updated on 19 November 202219 November 20...</td>\n",
       "      <td>['focal point', 'She does so much more than sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines        Source  \\\n",
       "0  Women’s Super League: talking points from the ...  The Guardian   \n",
       "1  Moving the Goalposts | ‘We want to keep dreami...  The Guardian   \n",
       "2  Leicester’s Ashleigh Plumptre: ‘I love everyth...  The Guardian   \n",
       "3       Man City win keeps pressure on WSL top three      BBC News   \n",
       "4      Shaw stars for Man City in WSL win at Everton      BBC News   \n",
       "\n",
       "                                           Author        Date  \\\n",
       "0  Suzanne Wrack, Sophie Downey and Sarah Rendell  2022-11-21   \n",
       "1                                   Sophie Downey  2022-12-07   \n",
       "2                                  Ella Braidwood  2022-11-17   \n",
       "3                                             NaN  2022-12-04   \n",
       "4                                             NaN  2022-11-19   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.theguardian.com/football/2022/nov/...   \n",
       "1  https://www.theguardian.com/football/2022/dec/...   \n",
       "2  https://www.theguardian.com/football/2022/nov/...   \n",
       "3      https://www.bbc.co.uk/sport/football/63771047   \n",
       "4      https://www.bbc.co.uk/sport/football/63606404   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Arsenal’s winning run was ended in dramatic st...   \n",
       "1  Club has soared since acquiring the Serie A li...   \n",
       "2  The defender on playing for the club in her he...   \n",
       "3  Last updated on 4 December 20224 December 2022...   \n",
       "4  Last updated on 19 November 202219 November 20...   \n",
       "\n",
       "                                              Quotes  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4  ['focal point', 'She does so much more than sc...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4338d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Content.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919aeaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Arsenal’s winning run was ended in dramatic st...\n",
       "1     Club has soared since acquiring the Serie A li...\n",
       "2     The defender on playing for the club in her he...\n",
       "3     Last updated on 4 December 20224 December 2022...\n",
       "4     Last updated on 19 November 202219 November 20...\n",
       "                            ...                        \n",
       "76    We use cookies and other tracking technologies...\n",
       "77    We use cookies and other tracking technologies...\n",
       "78    We use cookies and other tracking technologies...\n",
       "79    We use cookies and other tracking technologies...\n",
       "80    We use cookies and other tracking technologies...\n",
       "Name: Content, Length: 81, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Content.fillna('None') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "543ee86d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now I'm going to return to just using the previous DataFrame labelled 'df'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This includes both diary files + group and interview files\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# First I'll create a new column called 'tokenised_words'\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenised_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:8740\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8729\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8731\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   8732\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8733\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8738\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   8739\u001b[0m )\n\u001b[0;32m-> 8740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:688\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:812\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 812\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:828\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    827\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    830\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    831\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    832\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now I'm going to return to just using the previous DataFrame labelled 'df'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This includes both diary files + group and interview files\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# First I'll create a new column called 'tokenised_words'\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenised_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Now I'm going to return to just using the previous DataFrame labelled 'df'\n",
    "# This includes both diary files + group and interview files\n",
    "\n",
    "# First I'll create a new column called 'tokenised_words'\n",
    "\n",
    "df['tokenised_words'] = df.apply(lambda row: nltk.word_tokenize(row['Content']), axis = 1)\n",
    "\n",
    "# apply - used to apply a function along an axis of the DataFrame: i.e, axis 1\n",
    "# lambda - anonymous function (no name) that can take any number of arguments\n",
    "# lambda ensures that the function tokenize is applied to every ROW in the text column\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
